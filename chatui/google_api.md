Skip to main content
-

      [English](https://ai.google.dev/gemini-api/docs/video)

-
      [Deutsch](https://ai.google.dev/gemini-api/docs/video)

-
      [EspaÃ±ol â€“ AmÃ©rica Latina](https://ai.google.dev/gemini-api/docs/video)

-
      [FranÃ§ais](https://ai.google.dev/gemini-api/docs/video)

-
      [Indonesia](https://ai.google.dev/gemini-api/docs/video)

-
      [Italiano](https://ai.google.dev/gemini-api/docs/video)

-
      [Polski](https://ai.google.dev/gemini-api/docs/video)

-
      [PortuguÃªs â€“ Brasil](https://ai.google.dev/gemini-api/docs/video)

-
      [Shqip](https://ai.google.dev/gemini-api/docs/video)

-
      [TiÃªÌng ViÃªÌ£t](https://ai.google.dev/gemini-api/docs/video)

-
      [TÃ¼rkÃ§e](https://ai.google.dev/gemini-api/docs/video)

-
      [Ð ÑƒÑÑÐºÐ¸Ð¹](https://ai.google.dev/gemini-api/docs/video)

-
      [×¢×‘×¨×™×ª](https://ai.google.dev/gemini-api/docs/video)

-
      [Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‘Ø©](https://ai.google.dev/gemini-api/docs/video)

-
      [ÙØ§Ø±Ø³ÛŒ](https://ai.google.dev/gemini-api/docs/video)

-
      [à¤¹à¤¿à¤‚à¤¦à¥€](https://ai.google.dev/gemini-api/docs/video)

-
      [à¦¬à¦¾à¦‚à¦²à¦¾](https://ai.google.dev/gemini-api/docs/video)

-
      [à¸ à¸²à¸©à¸²à¹„à¸—à¸¢](https://ai.google.dev/gemini-api/docs/video)

-
      [ä¸­æ–‡ â€“ ç®€ä½“](https://ai.google.dev/gemini-api/docs/video)

-
      [ä¸­æ–‡ â€“ ç¹é«”](https://ai.google.dev/gemini-api/docs/video)

-
      [æ—¥æœ¬èªž](https://ai.google.dev/gemini-api/docs/video)

-
      [í•œêµ­ì–´](https://ai.google.dev/gemini-api/docs/video)

[English](https://ai.google.dev/gemini-api/docs/video)
[Deutsch](https://ai.google.dev/gemini-api/docs/video)
[EspaÃ±ol â€“ AmÃ©rica Latina](https://ai.google.dev/gemini-api/docs/video)
[FranÃ§ais](https://ai.google.dev/gemini-api/docs/video)
[Indonesia](https://ai.google.dev/gemini-api/docs/video)
[Italiano](https://ai.google.dev/gemini-api/docs/video)
[Polski](https://ai.google.dev/gemini-api/docs/video)
[PortuguÃªs â€“ Brasil](https://ai.google.dev/gemini-api/docs/video)
[Shqip](https://ai.google.dev/gemini-api/docs/video)
[TiÃªÌng ViÃªÌ£t](https://ai.google.dev/gemini-api/docs/video)
[TÃ¼rkÃ§e](https://ai.google.dev/gemini-api/docs/video)
[Ð ÑƒÑÑÐºÐ¸Ð¹](https://ai.google.dev/gemini-api/docs/video)
[×¢×‘×¨×™×ª](https://ai.google.dev/gemini-api/docs/video)
[Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‘Ø©](https://ai.google.dev/gemini-api/docs/video)
[ÙØ§Ø±Ø³ÛŒ](https://ai.google.dev/gemini-api/docs/video)
[à¤¹à¤¿à¤‚à¤¦à¥€](https://ai.google.dev/gemini-api/docs/video)
[à¦¬à¦¾à¦‚à¦²à¦¾](https://ai.google.dev/gemini-api/docs/video)
[à¸ à¸²à¸©à¸²à¹„à¸—à¸¢](https://ai.google.dev/gemini-api/docs/video)
[ä¸­æ–‡ â€“ ç®€ä½“](https://ai.google.dev/gemini-api/docs/video)
[ä¸­æ–‡ â€“ ç¹é«”](https://ai.google.dev/gemini-api/docs/video)
[æ—¥æœ¬èªž](https://ai.google.dev/gemini-api/docs/video)
[í•œêµ­ì–´](https://ai.google.dev/gemini-api/docs/video)
[Get API key](https://aistudio.google.com/apikey)
[Cookbook](https://github.com/google-gemini/cookbook)
[Community](https://discuss.ai.google.dev/c/gemini-api/)
[Docs](https://ai.google.dev/gemini-api/docs)
[API reference](https://ai.google.dev/api)
-

      Gemini API



















      Docs

  [API reference](https://ai.google.dev/api)

-

  [Docs](https://ai.google.dev/gemini-api/docs)

-

  [API reference](https://ai.google.dev/api)

-

  [Get API key](https://aistudio.google.com/apikey)

-

  [Cookbook](https://github.com/google-gemini/cookbook)

-

  [Community](https://discuss.ai.google.dev/c/gemini-api/)

[Gemini API](https://ai.google.dev/gemini-api/docs)
-

  [Docs](https://ai.google.dev/gemini-api/docs)

-

  [API reference](https://ai.google.dev/api)

[Docs](https://ai.google.dev/gemini-api/docs)
[API reference](https://ai.google.dev/api)
[Get API key](https://aistudio.google.com/apikey)
[Cookbook](https://github.com/google-gemini/cookbook)
[Community](https://discuss.ai.google.dev/c/gemini-api/)
-

        Get started

- [Overview](https://ai.google.dev/gemini-api/docs)
- [Quickstart](https://ai.google.dev/gemini-api/docs/quickstart)
- [API keys](https://ai.google.dev/gemini-api/docs/api-key)
- [Libraries](https://ai.google.dev/gemini-api/docs/libraries)
- [Interactions API](https://ai.google.dev/gemini-api/docs/interactions)
-
        Models

- [Gemini](https://ai.google.dev/gemini-api/docs/models)
- [Gemini 3](https://ai.google.dev/gemini-api/docs/gemini-3)

- [Nano banana (image generation)](https://ai.google.dev/gemini-api/docs/nanobanana)
- [Veo (video generation)](https://ai.google.dev/gemini-api/docs/video)
- [Lyria (music generation)](https://ai.google.dev/gemini-api/docs/music-generation)
- [Imagen (image generation)](https://ai.google.dev/gemini-api/docs/imagen)
- [Embeddings](https://ai.google.dev/gemini-api/docs/embeddings)
- [Robotics](https://ai.google.dev/gemini-api/docs/robotics-overview)
- [Pricing](https://ai.google.dev/gemini-api/docs/pricing)
- [Rate limits](https://ai.google.dev/gemini-api/docs/rate-limits)
-
        Core capabilities

- [Text](https://ai.google.dev/gemini-api/docs/text-generation)
-

        Image
      Image generation ðŸŒ[Image understanding](https://ai.google.dev/gemini-api/docs/image-understanding)

- [Image generation ðŸŒ](https://ai.google.dev/gemini-api/docs/image-generation)
- [Image understanding](https://ai.google.dev/gemini-api/docs/image-understanding)
- [Video](https://ai.google.dev/gemini-api/docs/video-understanding)
- [Documents](https://ai.google.dev/gemini-api/docs/document-processing)
-

        Speech and audio
      Speech generation[Audio understanding](https://ai.google.dev/gemini-api/docs/audio)

- [Speech generation](https://ai.google.dev/gemini-api/docs/speech-generation)
- [Audio understanding](https://ai.google.dev/gemini-api/docs/audio)
-

        Thinking and thought signatures
      Thinking[Thought signatures](https://ai.google.dev/gemini-api/docs/thought-signatures)

- [Thinking](https://ai.google.dev/gemini-api/docs/thinking)
- [Thought signatures](https://ai.google.dev/gemini-api/docs/thought-signatures)
- [Structured outputs](https://ai.google.dev/gemini-api/docs/structured-output)
- [Function calling](https://ai.google.dev/gemini-api/docs/function-calling)
- [Long context](https://ai.google.dev/gemini-api/docs/long-context)
-
        Tools and agents

- [Overview](https://ai.google.dev/gemini-api/docs/tools)
- [Deep Research](https://ai.google.dev/gemini-api/docs/deep-research)
- [Google Search](https://ai.google.dev/gemini-api/docs/google-search)
- [Google Maps](https://ai.google.dev/gemini-api/docs/maps-grounding)
- [Code execution](https://ai.google.dev/gemini-api/docs/code-execution)
- [URL context](https://ai.google.dev/gemini-api/docs/url-context)
- [Computer Use](https://ai.google.dev/gemini-api/docs/computer-use)
- [File Search](https://ai.google.dev/gemini-api/docs/file-search)
-
        Live API

- [Get started](https://ai.google.dev/gemini-api/docs/live)
- [Capabilities](https://ai.google.dev/gemini-api/docs/live-guide)
- [Tool use](https://ai.google.dev/gemini-api/docs/live-tools)
- [Session management](https://ai.google.dev/gemini-api/docs/live-session)
- [Ephemeral tokens](https://ai.google.dev/gemini-api/docs/ephemeral-tokens)
-
        Guides

- [Batch API](https://ai.google.dev/gemini-api/docs/batch-api)
- [Files API](https://ai.google.dev/gemini-api/docs/files)
- [Context caching](https://ai.google.dev/gemini-api/docs/caching)
- [OpenAI compatibility](https://ai.google.dev/gemini-api/docs/openai)
- [Media resolution](https://ai.google.dev/gemini-api/docs/media-resolution)
- [Token counting](https://ai.google.dev/gemini-api/docs/tokens)
- [Prompt engineering](https://ai.google.dev/gemini-api/docs/prompting-strategies)
-

        Logs and datasets
      Get started with logs[Data logging and sharing](https://ai.google.dev/gemini-api/docs/logs-policy)

- [Get started with logs](https://ai.google.dev/gemini-api/docs/logs-datasets)
- [Data logging and sharing](https://ai.google.dev/gemini-api/docs/logs-policy)
-

        Safety
      Safety settings[Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance)

- [Safety settings](https://ai.google.dev/gemini-api/docs/safety-settings)
- [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance)
-

        Open-Source frameworks
      LangChain & LangGraphCrewAILlamaIndex[Vercel AI SDK](https://ai.google.dev/gemini-api/docs/vercel-ai-sdk-example)

- [LangChain & LangGraph](https://ai.google.dev/gemini-api/docs/langgraph-example)
- [CrewAI](https://ai.google.dev/gemini-api/docs/crewai-example)
- [LlamaIndex](https://ai.google.dev/gemini-api/docs/llama-index)
- [Vercel AI SDK](https://ai.google.dev/gemini-api/docs/vercel-ai-sdk-example)
-
        Resources

- [Migrate to Gen AI SDK](https://ai.google.dev/gemini-api/docs/migrate)
- [Release notes](https://ai.google.dev/gemini-api/docs/changelog)

- [Deprecations](https://ai.google.dev/gemini-api/docs/deprecations)
- [API troubleshooting](https://ai.google.dev/gemini-api/docs/troubleshooting)
- [Billing info](https://ai.google.dev/gemini-api/docs/billing)
- [Partner and library integrations](https://ai.google.dev/gemini-api/docs/partner-integration)
-

        Google AI Studio
      QuickstartVibe code in Build modeTry out LearnLMTroubleshooting[Access for Workspace users](https://ai.google.dev/gemini-api/docs/workspace)

- [Quickstart](https://ai.google.dev/gemini-api/docs/ai-studio-quickstart)
- [Vibe code in Build mode](https://ai.google.dev/gemini-api/docs/aistudio-build-mode)
- [Try out LearnLM](https://ai.google.dev/gemini-api/docs/learnlm)
- [Troubleshooting](https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio)
- [Access for Workspace users](https://ai.google.dev/gemini-api/docs/workspace)
-

        Google Cloud Platform
      VertexAI Gemini API[OAuth authentication](https://ai.google.dev/gemini-api/docs/oauth)

- [VertexAI Gemini API](https://ai.google.dev/gemini-api/docs/migrate-to-cloud)
- [OAuth authentication](https://ai.google.dev/gemini-api/docs/oauth)
-
        Policies

- [Terms of service](https://ai.google.dev/gemini-api/terms)
- [Available regions](https://ai.google.dev/gemini-api/docs/available-regions)
- [Additional usage polices](https://ai.google.dev/gemini-api/docs/usage-policies)
[Overview](https://ai.google.dev/gemini-api/docs)
[Quickstart](https://ai.google.dev/gemini-api/docs/quickstart)
[API keys](https://ai.google.dev/gemini-api/docs/api-key)
[Libraries](https://ai.google.dev/gemini-api/docs/libraries)
[Interactions API](https://ai.google.dev/gemini-api/docs/interactions)
[Gemini](https://ai.google.dev/gemini-api/docs/models)
[Gemini 3](https://ai.google.dev/gemini-api/docs/gemini-3)
[Nano banana (image generation)](https://ai.google.dev/gemini-api/docs/nanobanana)
[Veo (video generation)](https://ai.google.dev/gemini-api/docs/video)
[Lyria (music generation)](https://ai.google.dev/gemini-api/docs/music-generation)
[Imagen (image generation)](https://ai.google.dev/gemini-api/docs/imagen)
[Embeddings](https://ai.google.dev/gemini-api/docs/embeddings)
[Robotics](https://ai.google.dev/gemini-api/docs/robotics-overview)
[Pricing](https://ai.google.dev/gemini-api/docs/pricing)
[Rate limits](https://ai.google.dev/gemini-api/docs/rate-limits)
[Text](https://ai.google.dev/gemini-api/docs/text-generation)
- [Image generation ðŸŒ](https://ai.google.dev/gemini-api/docs/image-generation)
- [Image understanding](https://ai.google.dev/gemini-api/docs/image-understanding)
[Image generation ðŸŒ](https://ai.google.dev/gemini-api/docs/image-generation)
[Image understanding](https://ai.google.dev/gemini-api/docs/image-understanding)
[Video](https://ai.google.dev/gemini-api/docs/video-understanding)
[Documents](https://ai.google.dev/gemini-api/docs/document-processing)
- [Speech generation](https://ai.google.dev/gemini-api/docs/speech-generation)
- [Audio understanding](https://ai.google.dev/gemini-api/docs/audio)
[Speech generation](https://ai.google.dev/gemini-api/docs/speech-generation)
[Audio understanding](https://ai.google.dev/gemini-api/docs/audio)
- [Thinking](https://ai.google.dev/gemini-api/docs/thinking)
- [Thought signatures](https://ai.google.dev/gemini-api/docs/thought-signatures)
[Thinking](https://ai.google.dev/gemini-api/docs/thinking)
[Thought signatures](https://ai.google.dev/gemini-api/docs/thought-signatures)
[Structured outputs](https://ai.google.dev/gemini-api/docs/structured-output)
[Function calling](https://ai.google.dev/gemini-api/docs/function-calling)
[Long context](https://ai.google.dev/gemini-api/docs/long-context)
[Overview](https://ai.google.dev/gemini-api/docs/tools)
[Deep Research](https://ai.google.dev/gemini-api/docs/deep-research)
[Google Search](https://ai.google.dev/gemini-api/docs/google-search)
[Google Maps](https://ai.google.dev/gemini-api/docs/maps-grounding)
[Code execution](https://ai.google.dev/gemini-api/docs/code-execution)
[URL context](https://ai.google.dev/gemini-api/docs/url-context)
[Computer Use](https://ai.google.dev/gemini-api/docs/computer-use)
[File Search](https://ai.google.dev/gemini-api/docs/file-search)
[Get started](https://ai.google.dev/gemini-api/docs/live)
[Capabilities](https://ai.google.dev/gemini-api/docs/live-guide)
[Tool use](https://ai.google.dev/gemini-api/docs/live-tools)
[Session management](https://ai.google.dev/gemini-api/docs/live-session)

[Ephemeral tokens](https://ai.google.dev/gemini-api/docs/ephemeral-tokens)
[Batch API](https://ai.google.dev/gemini-api/docs/batch-api)
[Files API](https://ai.google.dev/gemini-api/docs/files)
[Context caching](https://ai.google.dev/gemini-api/docs/caching)
[OpenAI compatibility](https://ai.google.dev/gemini-api/docs/openai)
[Media resolution](https://ai.google.dev/gemini-api/docs/media-resolution)
[Token counting](https://ai.google.dev/gemini-api/docs/tokens)
[Prompt engineering](https://ai.google.dev/gemini-api/docs/prompting-strategies)

- [Get started with logs](https://ai.google.dev/gemini-api/docs/logs-datasets)
- [Data logging and sharing](https://ai.google.dev/gemini-api/docs/logs-policy)
[Get started with logs](https://ai.google.dev/gemini-api/docs/logs-datasets)
[Data logging and sharing](https://ai.google.dev/gemini-api/docs/logs-policy)
- [Safety settings](https://ai.google.dev/gemini-api/docs/safety-settings)
- [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance)
[Safety settings](https://ai.google.dev/gemini-api/docs/safety-settings)
[Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance)
- [LangChain & LangGraph](https://ai.google.dev/gemini-api/docs/langgraph-example)
- [CrewAI](https://ai.google.dev/gemini-api/docs/crewai-example)
- [LlamaIndex](https://ai.google.dev/gemini-api/docs/llama-index)
- [Vercel AI SDK](https://ai.google.dev/gemini-api/docs/vercel-ai-sdk-example)
[LangChain & LangGraph](https://ai.google.dev/gemini-api/docs/langgraph-example)
[CrewAI](https://ai.google.dev/gemini-api/docs/crewai-example)
[LlamaIndex](https://ai.google.dev/gemini-api/docs/llama-index)
[Vercel AI SDK](https://ai.google.dev/gemini-api/docs/vercel-ai-sdk-example)
[Migrate to Gen AI SDK](https://ai.google.dev/gemini-api/docs/migrate)
[Release notes](https://ai.google.dev/gemini-api/docs/changelog)
[Deprecations](https://ai.google.dev/gemini-api/docs/deprecations)
[API troubleshooting](https://ai.google.dev/gemini-api/docs/troubleshooting)
[Billing info](https://ai.google.dev/gemini-api/docs/billing)
[Partner and library integrations](https://ai.google.dev/gemini-api/docs/partner-integration)
- [Quickstart](https://ai.google.dev/gemini-api/docs/ai-studio-quickstart)
- [Vibe code in Build mode](https://ai.google.dev/gemini-api/docs/aistudio-build-mode)
- [Try out LearnLM](https://ai.google.dev/gemini-api/docs/learnlm)
- [Troubleshooting](https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio)
- [Access for Workspace users](https://ai.google.dev/gemini-api/docs/workspace)
[Quickstart](https://ai.google.dev/gemini-api/docs/ai-studio-quickstart)
[Vibe code in Build mode](https://ai.google.dev/gemini-api/docs/aistudio-build-mode)
[Try out LearnLM](https://ai.google.dev/gemini-api/docs/learnlm)
[Troubleshooting](https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio)
[Access for Workspace users](https://ai.google.dev/gemini-api/docs/workspace)
- [VertexAI Gemini API](https://ai.google.dev/gemini-api/docs/migrate-to-cloud)
- [OAuth authentication](https://ai.google.dev/gemini-api/docs/oauth)
[VertexAI Gemini API](https://ai.google.dev/gemini-api/docs/migrate-to-cloud)
[OAuth authentication](https://ai.google.dev/gemini-api/docs/oauth)
[Terms of service](https://ai.google.dev/gemini-api/terms)
[Available regions](https://ai.google.dev/gemini-api/docs/available-regions)
[Additional usage polices](https://ai.google.dev/gemini-api/docs/usage-policies)
[Try it for free in Google AI Studio](https://aistudio.google.com?model=gemini-3-flash-preview)
-

  [Home](https://ai.google.dev/)

-

  [Gemini API](https://ai.google.dev/gemini-api)

-

  [Docs](https://ai.google.dev/gemini-api/docs)

[Home](https://ai.google.dev/)
[Gemini API](https://ai.google.dev/gemini-api)
[Docs](https://ai.google.dev/gemini-api/docs)

# Generate videos with Veo 3.1 in Gemini API

We have updated our [Terms of Service](https://ai.google.dev/gemini-api/terms).
Veo 3.1 is Google's state-of-the-art model for generating high-fidelity, 8-second 720p or 1080p videos featuring stunning realism and natively generated audio. You can access this model programmatically using the Gemini API. To learn more about the available Veo model variants, see the Model Versions section.
Veo 3.1 excels at a wide range of visual and cinematic styles and introduces several new capabilities:

- Video extension: Extend videos that were previously
generated using Veo.
- Frame-specific generation: Generate a video by
specifying the first and last frames.
- Image-based direction: Use up to three reference images to guide
the content of your generated video.
For more information about writing effective text prompts for video generation, see the Veo prompt guide

## Text to video generation

Choose an example to see how to generate a video with dialogue, cinematic realism, or creative animation:
Dialogue & Sound Effects Cinematic Realism Creative Animation

### Python

```
import time from google import genai from google.genai import types client = genai.Client() prompt = """A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'""" operation = client.models.generate_videos( model="veo-3.1-generate-preview", prompt=prompt, ) # Poll the operation status until the video is ready. while not operation.done: print("Waiting for video generation to complete...") time.sleep(10) operation = client.operations.get(operation) # Download the generated video. generated_video = operation.response.generated_videos[0] client.files.download(file=generated_video.video) generated_video.video.save("dialogue_example.mp4") print("Generated video saved to dialogue_example.mp4")
```

```
import time from google import genai from google.genai import types client = genai.Client() prompt = """A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'""" operation = client.models.generate_videos( model="veo-3.1-generate-preview", prompt=prompt, ) # Poll the operation status until the video is ready. while not operation.done: print("Waiting for video generation to complete...") time.sleep(10) operation = client.operations.get(operation) # Download the generated video. generated_video = operation.response.generated_videos[0] client.files.download(file=generated_video.video) generated_video.video.save("dialogue_example.mp4") print("Generated video saved to dialogue_example.mp4")
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai"; const ai = new GoogleGenAI({}); const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`; let operation = await ai.models.generateVideos({ model: "veo-3.1-generate-preview", prompt: prompt, }); // Poll the operation status until the video is ready. while (!operation.done) { console.log("Waiting for video generation to complete...") await new Promise((resolve) => setTimeout(resolve, 10000)); operation = await ai.operations.getVideosOperation({ operation: operation, }); } // Download the generated video. ai.files.download({ file: operation.response.generatedVideos[0].video, downloadPath: "dialogue_example.mp4", }); console.log(`Generated video saved to dialogue_example.mp4`);
```

```
import { GoogleGenAI } from "@google/genai"; const ai = new GoogleGenAI({}); const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`; let operation = await ai.models.generateVideos({ model: "veo-3.1-generate-preview", prompt: prompt, }); // Poll the operation status until the video is ready. while (!operation.done) { console.log("Waiting for video generation to complete...") await new Promise((resolve) => setTimeout(resolve, 10000)); operation = await ai.operations.getVideosOperation({ operation: operation, }); } // Download the generated video. ai.files.download({ file: operation.response.generatedVideos[0].video, downloadPath: "dialogue_example.mp4", }); console.log(`Generated video saved to dialogue_example.mp4`);
```

### Go

```
package main import ( "context" "log" "os" "time" "google.golang.org/genai" ) func main() { ctx := context.Background() client, err := genai.NewClient(ctx, nil) if err != nil { log.Fatal(err) } prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'` operation, _ := client.Models.GenerateVideos( ctx, "veo-3.1-generate-preview", prompt, nil, nil, ) // Poll the operation status until the video is ready. for !operation.Done { log.Println("Waiting for video generation to complete...") time.Sleep(10 * time.Second) operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil) } // Download the generated video. video := operation.Response.GeneratedVideos[0] client.Files.Download(ctx, video.Video, nil) fname := "dialogue_example.mp4" _ = os.WriteFile(fname, video.Video.VideoBytes, 0644) log.Printf("Generated video saved to %s\n", fname) }
```

```
package main import ( "context" "log" "os" "time" "google.golang.org/genai" ) func main() { ctx := context.Background() client, err := genai.NewClient(ctx, nil) if err != nil { log.Fatal(err) } prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'` operation, _ := client.Models.GenerateVideos( ctx, "veo-3.1-generate-preview", prompt, nil, nil, ) // Poll the operation status until the video is ready. for !operation.Done { log.Println("Waiting for video generation to complete...") time.Sleep(10 * time.Second) operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil) } // Download the generated video. video := operation.Response.GeneratedVideos[0] client.Files.Download(ctx, video.Video, nil) fname := "dialogue_example.mp4" _ = os.WriteFile(fname, video.Video.VideoBytes, 0644) log.Printf("Generated video saved to %s\n", fname) }
```

### REST

```
# Note: This script uses jq to parse the JSON response. # GEMINI API Base URL BASE_URL="https://generativelanguage.googleapis.com/v1beta" # Send request to generate video and capture the operation name into a variable. operation_name=$(curl -s "${BASE_URL}/models/veo-3.1-generate-preview:predictLongRunning" \ -H "x-goog-api-key: $GEMINI_API_KEY" \ -H "Content-Type: application/json" \ -X "POST" \ -d '{ "instances": [{ "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That'\''s the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\"" } ] }' | jq -r .name) # Poll the operation status until the video is ready while true; do # Get the full JSON status and store it in a variable. status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}") # Check the "done" field from the JSON stored in the variable. is_done=$(echo "${status_response}" | jq .done) if [ "${is_done}" = "true" ]; then # Extract the download URI from the final response. video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri') echo "Downloading video from: ${video_uri}" # Download the video using the URI and API key and follow redirects. curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}" break fi # Wait for 5 seconds before checking again. sleep 10 done
```

```
# Note: This script uses jq to parse the JSON response. # GEMINI API Base URL BASE_URL="https://generativelanguage.googleapis.com/v1beta" # Send request to generate video and capture the operation name into a variable. operation_name=$(curl -s "${BASE_URL}/models/veo-3.1-generate-preview:predictLongRunning" \ -H "x-goog-api-key: $GEMINI_API_KEY" \ -H "Content-Type: application/json" \ -X "POST" \ -d '{ "instances": [{ "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That'\''s the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\"" } ] }' | jq -r .name) # Poll the operation status until the video is ready while true; do # Get the full JSON status and store it in a variable. status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}") # Check the "done" field from the JSON stored in the variable. is_done=$(echo "${status_response}" | jq .done) if [ "${is_done}" = "true" ]; then # Extract the download URI from the final response. video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri') echo "Downloading video from: ${video_uri}" # Download the video using the URI and API key and follow redirects. curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}" break fi # Wait for 5 seconds before checking again. sleep 10 done
```

## Image to video generation

The following code demonstrates generating an image using [Gemini 2.5 Flash Image aka Nano Banana](https://ai.google.dev/gemini-api/docs/image-generation), then using that image as the starting frame for generating a video with Veo 3.1.

### Python

```
import time from google import genai client = genai.Client() prompt = "Panning wide shot of a calico kitten sleeping in the sunshine" # Step 1: Generate an image with Nano Banana. image = client.models.generate_content( model="gemini-2.5-flash-image", contents=prompt, config={"response_modalities":['IMAGE']} ) # Step 2: Generate video with Veo 3.1 using the image. operation = client.models.generate_videos( model="veo-3.1-generate-preview", prompt=prompt, image=image.parts[0].as_image(), ) # Poll the operation status until the video is ready. while not operation.done: print("Waiting for video generation to complete...") time.sleep(10) operation = client.operations.get(operation) # Download the video. video = operation.response.generated_videos[0] client.files.download(file=video.video) video.video.save("veo3_with_image_input.mp4") print("Generated video saved to veo3_with_image_input.mp4")
```

```
import time from google import genai client = genai.Client() prompt = "Panning wide shot of a calico kitten sleeping in the sunshine" # Step 1: Generate an image with Nano Banana. image = client.models.generate_content( model="gemini-2.5-flash-image", contents=prompt, config={"response_modalities":['IMAGE']} ) # Step 2: Generate video with Veo 3.1 using the image. operation = client.models.generate_videos( model="veo-3.1-generate-preview", prompt=prompt, image=image.parts[0].as_image(), ) # Poll the operation status until the video is ready. while not operation.done: print("Waiting for video generation to complete...") time.sleep(10) operation = client.operations.get(operation) # Download the video. video = operation.response.generated_videos[0] client.files.download(file=video.video) video.video.save("veo3_with_image_input.mp4") print("Generated video saved to veo3_with_image_input.mp4")
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai"; const ai = new GoogleGenAI({}); const prompt = "Panning wide shot of a calico kitten sleeping in the sunshine"; // Step 1: Generate an image with Nano Banana. const imageResponse = await ai.models.generateContent({ model: "gemini-2.5-flash-image", prompt: prompt, }); // Step 2: Generate video with Veo 3.1 using the image. let operation = await ai.models.generateVideos({ model: "veo-3.1-generate-preview", prompt: prompt, image: { imageBytes: imageResponse.generatedImages[0].image.imageBytes, mimeType: "image/png", }, }); // Poll the operation status until the video is ready. while (!operation.done) { console.log("Waiting for video generation to complete...") await new Promise((resolve) => setTimeout(resolve, 10000)); operation = await ai.operations.getVideosOperation({ operation: operation, }); } // Download the video. ai.files.download({ file: operation.response.generatedVideos[0].video, downloadPath: "veo3_with_image_input.mp4", }); console.log(`Generated video saved to veo3_with_image_input.mp4`);
```

```
import { GoogleGenAI } from "@google/genai"; const ai = new GoogleGenAI({}); const prompt = "Panning wide shot of a calico kitten sleeping in the sunshine"; // Step 1: Generate an image with Nano Banana. const imageResponse = await ai.models.generateContent({ model: "gemini-2.5-flash-image", prompt: prompt, }); // Step 2: Generate video with Veo 3.1 using the image. let operation = await ai.models.generateVideos({ model: "veo-3.1-generate-preview", prompt: prompt, image: { imageBytes: imageResponse.generatedImages[0].image.imageBytes, mimeType: "image/png", }, }); // Poll the operation status until the video is ready. while (!operation.done) { console.log("Waiting for video generation to complete...") await new Promise((resolve) => setTimeout(resolve, 10000)); operation = await ai.operations.getVideosOperation({ operation: operation, }); } // Download the video. ai.files.download({ file: operation.response.generatedVideos[0].video, downloadPath: "veo3_with_image_input.mp4", }); console.log(`Generated video saved to veo3_with_image_input.mp4`);
```

### Go

```
package main import ( "context" "log" "os" "time" "google.golang.org/genai" ) func main() { ctx := context.Background() client, err := genai.NewClient(ctx, nil) if err != nil { log.Fatal(err) } prompt := "Panning wide shot of a calico kitten sleeping in the sunshine" // Step 1: Generate an image with Nano Banana. imageResponse, err := client.Models.GenerateContent( ctx, "gemini-2.5-flash-image", prompt, nil, // GenerateImagesConfig ) if err != nil { log.Fatal(err) } // Step 2: Generate video with Veo 3.1 using the image. operation, err := client.Models.GenerateVideos( ctx, "veo-3.1-generate-preview", prompt, imageResponse.GeneratedImages[0].Image, nil, // GenerateVideosConfig ) if err != nil { log.Fatal(err) } // Poll the operation status until the video is ready. for !operation.Done { log.Println("Waiting for video generation to complete...") time.Sleep(10 * time.Second) operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil) } // Download the video. video := operation.Response.GeneratedVideos[0] client.Files.Download(ctx, video.Video, nil) fname := "veo3_with_image_input.mp4" _ = os.WriteFile(fname, video.Video.VideoBytes, 0644) log.Printf("Generated video saved to %s\n", fname) }
```

```
package main import ( "context" "log" "os" "time" "google.golang.org/genai" ) func main() { ctx := context.Background() client, err := genai.NewClient(ctx, nil) if err != nil { log.Fatal(err) } prompt := "Panning wide shot of a calico kitten sleeping in the sunshine" // Step 1: Generate an image with Nano Banana. imageResponse, err := client.Models.GenerateContent( ctx, "gemini-2.5-flash-image", prompt, nil, // GenerateImagesConfig ) if err != nil { log.Fatal(err) } // Step 2: Generate video with Veo 3.1 using the image. operation, err := client.Models.GenerateVideos( ctx, "veo-3.1-generate-preview", prompt, imageResponse.GeneratedImages[0].Image, nil, // GenerateVideosConfig ) if err != nil { log.Fatal(err) } // Poll the operation status until the video is ready. for !operation.Done { log.Println("Waiting for video generation to complete...") time.Sleep(10 * time.Second) operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil) } // Download the video. video := operation.Response.GeneratedVideos[0] client.Files.Download(ctx, video.Video, nil) fname := "veo3_with_image_input.mp4" _ = os.WriteFile(fname, video.Video.VideoBytes, 0644) log.Printf("Generated video saved to %s\n", fname) }
```

### Using reference images

Veo 3.1 now accepts up to 3 reference images to guide your generated video's content. Provide images of a person, character, or product to preserve the subject's appearance in the output video.
For example, using these three images generated with Nano Banana as references with a well-written prompt creates the following video:

```
`dress_image`
```

```
`woman_image`
```

```
`glasses_image`
```

### Python

```
import time from google import genai client = genai.Client() prompt = "The video opens with a medium, eye-level shot of a beautiful woman with dark hair and warm brown eyes. She wears a magnificent, high-fashion flamingo dress with layers of pink and fuchsia feathers, complemented by whimsical pink, heart-shaped sunglasses. She walks with serene confidence through the crystal-clear, shallow turquoise water of a sun-drenched lagoon. The camera slowly pulls back to a medium-wide shot, revealing the breathtaking scene as the dress's long train glides and floats gracefully on the water's surface behind her. The cinematic, dreamlike atmosphere is enhanced by the vibrant colors of the dress against the serene, minimalist landscape, capturing a moment of pure elegance and high-fashion fantasy." dress_reference = types.VideoGenerationReferenceImage( image=dress_image, # Generated separately with Nano Banana reference_type="asset" ) sunglasses_reference = types.VideoGenerationReferenceImage( image=glasses_image, # Generated separately with Nano Banana reference_type="asset" ) woman_reference = types.VideoGenerationReferenceImage( image=woman_image, # Generated separately with Nano Banana reference_type="asset" ) operation = client.models.generate_videos( model="veo-3.1-generate-preview", prompt=prompt, config=types.GenerateVideosConfig( reference_images=[dress_reference, glasses_reference, woman_reference], ), ) # Poll the operation status until the video is ready. while not operation.done: print("Waiting for video generation to complete...") time.sleep(10) operation = client.operations.get(operation) # Download the video. video = operation.response.generated_videos[0] client.files.download(file=video.video) video.video.save("veo3.1_with_reference_images.mp4") print("Generated video saved to veo3.1_with_reference_images.mp4")
```

```
import time from google import genai client = genai.Client() prompt = "The video opens with a medium, eye-level shot of a beautiful woman with dark hair and warm brown eyes. She wears a magnificent, high-fashion flamingo dress with layers of pink and fuchsia feathers, complemented by whimsical pink, heart-shaped sunglasses. She walks with serene confidence through the crystal-clear, shallow turquoise water of a sun-drenched lagoon. The camera slowly pulls back to a medium-wide shot, revealing the breathtaking scene as the dress's long train glides and floats gracefully on the water's surface behind her. The cinematic, dreamlike atmosphere is enhanced by the vibrant colors of the dress against the serene, minimalist landscape, capturing a moment of pure elegance and high-fashion fantasy." dress_reference = types.VideoGenerationReferenceImage( image=dress_image, # Generated separately with Nano Banana reference_type="asset" ) sunglasses_reference = types.VideoGenerationReferenceImage( image=glasses_image, # Generated separately with Nano Banana reference_type="asset" ) woman_reference = types.VideoGenerationReferenceImage( image=woman_image, # Generated separately with Nano Banana reference_type="asset" ) operation = client.models.generate_videos( model="veo-3.1-generate-preview", prompt=prompt, config=types.GenerateVideosConfig( reference_images=[dress_reference, glasses_reference, woman_reference], ), ) # Poll the operation status until the video is ready. while not operation.done: print("Waiting for video generation to complete...") time.sleep(10) operation = client.operations.get(operation) # Download the video. video = operation.response.generated_videos[0] client.files.download(file=video.video) video.video.save("veo3.1_with_reference_images.mp4") print("Generated video saved to veo3.1_with_reference_images.mp4")
```

### Using first and last frames

Veo 3.1 lets you create videos using interpolation, or specifying the first and last frames of the video. For information about writing effective text prompts for video generation, see the Veo prompt guide.

### Python

```
import time from google import genai client = genai.Client() prompt = "A cinematic, haunting video. A ghostly woman with long white hair and a flowing dress swings gently on a rope swing beneath a massive, gnarled tree in a foggy, moonlit clearing. The fog thickens and swirls around her, and she slowly fades away, vanishing completely. The empty swing is left swaying rhythmically on its own in the eerie silence." operation = client.models.generate_videos( model="veo-3.1-generate-preview", prompt=prompt, image=first_image, # Generated separately with Nano Banana config=types.GenerateVideosConfig( last_frame=last_image # Generated separately with Nano Banana ), ) # Poll the operation status until the video is ready. while not operation.done: print("Waiting for video generation to complete...") time.sleep(10) operation = client.operations.get(operation) # Download the video. video = operation.response.generated_videos[0] client.files.download(file=video.video) video.video.save("veo3.1_with_interpolation.mp4") print("Generated video saved to veo3.1_with_interpolation.mp4")
```

```
import time from google import genai client = genai.Client() prompt = "A cinematic, haunting video. A ghostly woman with long white hair and a flowing dress swings gently on a rope swing beneath a massive, gnarled tree in a foggy, moonlit clearing. The fog thickens and swirls around her, and she slowly fades away, vanishing completely. The empty swing is left swaying rhythmically on its own in the eerie silence." operation = client.models.generate_videos( model="veo-3.1-generate-preview", prompt=prompt, image=first_image, # Generated separately with Nano Banana config=types.GenerateVideosConfig( last_frame=last_image # Generated separately with Nano Banana ), ) # Poll the operation status until the video is ready. while not operation.done: print("Waiting for video generation to complete...") time.sleep(10) operation = client.operations.get(operation) # Download the video. video = operation.response.generated_videos[0] client.files.download(file=video.video) video.video.save("veo3.1_with_interpolation.mp4") print("Generated video saved to veo3.1_with_interpolation.mp4")
```

```
`first_image`
```

```
`last_image`
```

## Extending Veo videos

Use Veo 3.1 to extend videos that you previously generated with Veo by 7 seconds and up to 20 times.
Input video limitations:

- Veo-generated videos only up to 141 seconds long.
- Gemini API only supports video extensions for Veo-generated videos.
- The video should come from a previous generation, like operation.response.generated_videos[0].video
- Input videos are expected to have a certain length, aspect ratio, and dimensions:

Aspect ratio: 9:16 or 16:9
Resolution: 720p
Video length: 141 seconds or less

- Aspect ratio: 9:16 or 16:9
- Resolution: 720p
- Video length: 141 seconds or less

```
operation.response.generated_videos[0].video
```

- Aspect ratio: 9:16 or 16:9
- Resolution: 720p
- Video length: 141 seconds or less
The output of the extension is a single video combining the user input video and the generated extended video for up to 148 seconds of video.
This example takes the a Veo-generated video, shown here with its original prompt, and extends it using the video parameter and a new prompt:

```
video
```

```
butterfly_video
```

### Python

```
import time from google import genai client = genai.Client() prompt = "Track the butterfly into the garden as it lands on an orange origami flower. A fluffy white puppy runs up and gently pats the flower." operation = client.models.generate_videos( model="veo-3.1-generate-preview", video=operation.response.generated_videos[0].video, # This must be a video from a previous generation prompt=prompt, config=types.GenerateVideosConfig( number_of_videos=1, resolution="720p" ), ) # Poll the operation status until the video is ready. while not operation.done: print("Waiting for video generation to complete...") time.sleep(10) operation = client.operations.get(operation) # Download the video. video = operation.response.generated_videos[0] client.files.download(file=video.video) video.video.save("veo3.1_extension.mp4") print("Generated video saved to veo3.1_extension.mp4")
```

```
import time from google import genai client = genai.Client() prompt = "Track the butterfly into the garden as it lands on an orange origami flower. A fluffy white puppy runs up and gently pats the flower." operation = client.models.generate_videos( model="veo-3.1-generate-preview", video=operation.response.generated_videos[0].video, # This must be a video from a previous generation prompt=prompt, config=types.GenerateVideosConfig( number_of_videos=1, resolution="720p" ), ) # Poll the operation status until the video is ready. while not operation.done: print("Waiting for video generation to complete...") time.sleep(10) operation = client.operations.get(operation) # Download the video. video = operation.response.generated_videos[0] client.files.download(file=video.video) video.video.save("veo3.1_extension.mp4") print("Generated video saved to veo3.1_extension.mp4")
```

For information about writing effective text prompts for video generation, see the Veo prompt guide.

## Handling asynchronous operations

Video generation is a computationally intensive task. When you send a request to the API, it starts a long-running job and immediately returns an operation object. You must then poll until the video is ready, which is indicated by the done status being true.

```
operation
```

```
done
```

The core of this process is a polling loop, which periodically checks the job's status.

### Python

```
import time from google import genai from google.genai import types client = genai.Client() # After starting the job, you get an operation object. operation = client.models.generate_videos( model="veo-3.1-generate-preview", prompt="A cinematic shot of a majestic lion in the savannah.", ) # Alternatively, you can use operation.name to get the operation. operation = types.GenerateVideosOperation(name=operation.name) # This loop checks the job status every 10 seconds. while not operation.done: time.sleep(10) # Refresh the operation object to get the latest status. operation = client.operations.get(operation) # Once done, the result is in operation.response. # ... process and download your video ...
```

```
import time from google import genai from google.genai import types client = genai.Client() # After starting the job, you get an operation object. operation = client.models.generate_videos( model="veo-3.1-generate-preview", prompt="A cinematic shot of a majestic lion in the savannah.", ) # Alternatively, you can use operation.name to get the operation. operation = types.GenerateVideosOperation(name=operation.name) # This loop checks the job status every 10 seconds. while not operation.done: time.sleep(10) # Refresh the operation object to get the latest status. operation = client.operations.get(operation) # Once done, the result is in operation.response. # ... process and download your video ...
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai"; const ai = new GoogleGenAI({}); // After starting the job, you get an operation object. let operation = await ai.models.generateVideos({ model: "veo-3.1-generate-preview", prompt: "A cinematic shot of a majestic lion in the savannah.", }); // Alternatively, you can use operation.name to get the operation. // operation = types.GenerateVideosOperation(name=operation.name) // This loop checks the job status every 10 seconds. while (!operation.done) { await new Promise((resolve) => setTimeout(resolve, 1000)); // Refresh the operation object to get the latest status. operation = await ai.operations.getVideosOperation({ operation }); } // Once done, the result is in operation.response. // ... process and download your video ...
```

```
import { GoogleGenAI } from "@google/genai"; const ai = new GoogleGenAI({}); // After starting the job, you get an operation object. let operation = await ai.models.generateVideos({ model: "veo-3.1-generate-preview", prompt: "A cinematic shot of a majestic lion in the savannah.", }); // Alternatively, you can use operation.name to get the operation. // operation = types.GenerateVideosOperation(name=operation.name) // This loop checks the job status every 10 seconds. while (!operation.done) { await new Promise((resolve) => setTimeout(resolve, 1000)); // Refresh the operation object to get the latest status. operation = await ai.operations.getVideosOperation({ operation }); } // Once done, the result is in operation.response. // ... process and download your video ...
```

## Veo API parameters and specifications

These are the parameters you can set in your API request to control the video generation process.

```
prompt
```

```
string
```

```
string
```

```
string
```

```
negativePrompt
```

```
string
```

```
string
```

```
string
```

```
image
```

```
Image
```

```
Image
```

```
Image
```

```
lastFrame
```

```
image
```

```
Image
```

```
Image
```

```
Image
```

```
referenceImages
```

```
VideoGenerationReferenceImage
```

```
video
```

```
Video
```

```
aspectRatio
```

```
"16:9"
```

```
"9:16"
```

```
"16:9"
```

```
"9:16"
```

```
"16:9"
```

```
"9:16"
```

```
resolution
```

```
"720p"
```

```
"1080p"
```

```
"720p"
```

```
"720p"
```

```
"1080p"
```

```
durationSeconds
```

```
"4"
```

```
"6"
```

```
"8"
```

```
referenceImages
```

```
"4"
```

```
"6"
```

```
"8"
```

```
"5"
```

```
"6"
```

```
"8"
```

```
personGeneration
```

Limitations

```
"allow_all"
```

```
"allow_adult"
```

```
"allow_all"
```

```
"allow_adult"
```

```
"allow_all"
```

```
"allow_adult"
```

```
"dont_allow"
```

```
"allow_adult"
```

```
"dont_allow"
```

Note that the seed parameter is also available for Veo 3 models. It doesn't guarantee determinism, but slightly improves it.

```
seed
```

You can customize your video generation by setting parameters in your request. For example you can specify negativePrompt to guide the model.

```
negativePrompt
```

### Python

```python
import time from google import genai from google.genai import types client = genai.Client() operation = client.models.generate_videos( model="veo-3.1-generate-preview", prompt="A cinematic shot of a majestic lion in the savannah.", config=types.GenerateVideosConfig(negative_prompt="cartoon, drawing, low quality"), ) # Poll the operation status until the video is ready. while not operation.done: print("Waiting for video generation to complete...") time.sleep(10) operation = client.operations.get(operation) # Download the generated video. generated_video = operation.response.generated_videos[0] client.files.download(file=generated_video.video) generated_video.video.save("parameters_example.mp4") print("Generated video saved to parameters_example.mp4")
```

```python
import time from google import genai from google.genai import types client = genai.Client() operation = client.models.generate_videos( model="veo-3.1-generate-preview", prompt="A cinematic shot of a majestic lion in the savannah.", config=types.GenerateVideosConfig(negative_prompt="cartoon, drawing, low quality"), ) # Poll the operation status until the video is ready. while not operation.done: print("Waiting for video generation to complete...") time.sleep(10) operation = client.operations.get(operation) # Download the generated video. generated_video = operation.response.generated_videos[0] client.files.download(file=generated_video.video) generated_video.video.save("parameters_example.mp4") print("Generated video saved to parameters_example.mp4")
```

### JavaScript

```javascript
import { GoogleGenAI } from "@google/genai"; const ai = new GoogleGenAI({}); let operation = await ai.models.generateVideos({ model: "veo-3.1-generate-preview", prompt: "A cinematic shot of a majestic lion in the savannah.", config: { aspectRatio: "16:9", negativePrompt: "cartoon, drawing, low quality" }, }); // Poll the operation status until the video is ready. while (!operation.done) { console.log("Waiting for video generation to complete...") await new Promise((resolve) => setTimeout(resolve, 10000)); operation = await ai.operations.getVideosOperation({ operation: operation, }); } // Download the generated video. ai.files.download({ file: operation.response.generatedVideos[0].video, downloadPath: "parameters_example.mp4", }); console.log(`Generated video saved to parameters_example.mp4`);
```

```javascript
import { GoogleGenAI } from "@google/genai"; const ai = new GoogleGenAI({}); let operation = await ai.models.generateVideos({ model: "veo-3.1-generate-preview", prompt: "A cinematic shot of a majestic lion in the savannah.", config: { aspectRatio: "16:9", negativePrompt: "cartoon, drawing, low quality" }, }); // Poll the operation status until the video is ready. while (!operation.done) { console.log("Waiting for video generation to complete...") await new Promise((resolve) => setTimeout(resolve, 10000)); operation = await ai.operations.getVideosOperation({ operation: operation, }); } // Download the generated video. ai.files.download({ file: operation.response.generatedVideos[0].video, downloadPath: "parameters_example.mp4", }); console.log(`Generated video saved to parameters_example.mp4`);
```

### Go

```go
package main import ( "context" "log" "os" "time" "google.golang.org/genai" ) func main() { ctx := context.Background() client, err := genai.NewClient(ctx, nil) if err != nil { log.Fatal(err) } videoConfig := &genai.GenerateVideosConfig{ AspectRatio: "16:9", NegativePrompt: "cartoon, drawing, low quality", } operation, _ := client.Models.GenerateVideos( ctx, "veo-3.1-generate-preview", "A cinematic shot of a majestic lion in the savannah.", nil, videoConfig, ) // Poll the operation status until the video is ready. for !operation.Done { log.Println("Waiting for video generation to complete...") time.Sleep(10 * time.Second) operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil) } // Download the generated video. video := operation.Response.GeneratedVideos[0] client.Files.Download(ctx, video.Video, nil) fname := "parameters_example.mp4" _ = os.WriteFile(fname, video.Video.VideoBytes, 0644) log.Printf("Generated video saved to %s\n", fname) }
```

```go
package main import ( "context" "log" "os" "time" "google.golang.org/genai" ) func main() { ctx := context.Background() client, err := genai.NewClient(ctx, nil) if err != nil { log.Fatal(err) } videoConfig := &genai.GenerateVideosConfig{ AspectRatio: "16:9", NegativePrompt: "cartoon, drawing, low quality", } operation, _ := client.Models.GenerateVideos( ctx, "veo-3.1-generate-preview", "A cinematic shot of a majestic lion in the savannah.", nil, videoConfig, ) // Poll the operation status until the video is ready. for !operation.Done { log.Println("Waiting for video generation to complete...") time.Sleep(10 * time.Second) operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil) } // Download the generated video. video := operation.Response.GeneratedVideos[0] client.Files.Download(ctx, video.Video, nil) fname := "parameters_example.mp4" _ = os.WriteFile(fname, video.Video.VideoBytes, 0644) log.Printf("Generated video saved to %s\n", fname) }
```

### REST

```bash
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL BASE_URL="https://generativelanguage.googleapis.com/v1beta"
# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.1-generate-preview:predictLongRunning" \ -H "x-goog-api-key: $GEMINI_API_KEY" \ -H "Content-Type: application/json" \ -X "POST" \ -d '{ "instances": [{ "prompt": "A cinematic shot of a majestic lion in the savannah." } ], "parameters": { "aspectRatio": "16:9", "negativePrompt": "cartoon, drawing, low quality" } }' | jq -r .name)
# Poll the operation status until the video is ready while true; do # Get the full JSON status and store it in a variable. status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}") # Check the "done" field from the JSON stored in the variable. is_done=$(echo "${status_response}" | jq .done) if [ "${is_done}" = "true" ]; then # Extract the download URI from the final response. video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri') echo "Downloading video from: ${video_uri}" # Download the video using the URI and API key and follow redirects. curl -L -o parameters_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}" break fi # Wait for 5 seconds before checking again. sleep 10 done
```

## Veo prompt guide

This section contains examples of videos you can create using Veo, and shows you how to modify prompts to produce distinct results.

### Safety filters

Veo applies safety filters across Gemini to help ensure that generated videos and uploaded photos don't contain offensive content. Prompts that violate our [terms and guidelines](https://ai.google.dev/gemini-api/docs/usage-policies) are blocked.

### Prompt writing basics

Good prompts are descriptive and clear. To get the most out of Veo, start with identifying your core idea, refine your idea by adding keywords and modifiers, and incorporate video-specific terminology into your prompts.
The following elements should be included in your prompt:

- Subject: The object, person, animal, or scenery that you want in your
video, such as cityscape, nature, vehicles, or puppies.
- Action: What the subject is doing (for example, walking, running, or
turning their head).
- Style: Specify creative direction using specific film
style keywords, such as sci-fi, horror film, film noir, or animated
styles like cartoon.
- Camera positioning and motion: [Optional] Control the camera's location
and movement using terms like aerial view, eye-level, top-down shot,
dolly shot, or worms eye.
- Composition: [Optional] How the shot is framed, such as wide shot,
close-up, single-shot or two-shot.
- Focus and lens effects: [Optional] Use terms like shallow focus,
deep focus, soft focus, macro lens, and wide-angle lens to achieve
specific visual effects.
- Ambiance: [Optional] How the color and light contribute to the scene,
such as blue tones, night, or warm tones.

#### More tips for writing prompts

- Use descriptive language: Use adjectives and adverbs to paint a clear
picture for Veo.
- Enhance the facial details: Specify
facial details as a focus of the photo like using the word portrait in
the prompt.
For more comprehensive prompting strategies, visit [Introduction to prompt design](https://ai.google.dev/gemini-api/docs/prompting-intro).

### Prompting for audio

With Veo 3, you can provide cues for sound effects, ambient noise, and dialogue. The model captures the nuance of these cues to generate a synchronized soundtrack.

- Dialogue: Use quotes for specific speech. (Example: "This must be the
key," he murmured.)
- Sound Effects (SFX): Explicitly describe sounds. (Example: tires
screeching loudly, engine roaring.)
- Ambient Noise: Describe the environment's soundscape. (Example: A faint,
eerie hum resonates in the background.)
These videos demonstrate prompting Veo 3's audio generation with increasing levels of detail.
Try out these prompts yourself to hear the audio! [Try Veo 3](https://deepmind.google/models/veo/)

### Prompting with reference images

You can use one or more images as inputs to guide your generated videos, using Veo's [image-to-video](https://ai.google.dev/gemini-api/docs/video) capabilities. Veo uses the input image as the initial frame. Select an image closest to what you envision as the first scene of your video to animate everyday objects, bring drawings and paintings to life, and add movement and sound to nature scenes.
Veo 3.1 lets you reference images or ingredients to direct your generated video's content. Provide up to three asset images of a single person, character, or product. Veo preserves the subject's appearance in the output video.
Using Veo 3.1, you can also generate videos by specifying the first and last frames of the video.
This feature gives you precise control over your shot's composition by letting you define the starting and ending frame. Upload an image or use a frame from a previous video generation to make sure your scene begins and concludes exactly as you envision it.

### Prompting for extension

To extend your Veo-generated video with Veo 3.1, use the video as an input along with an optional text prompt. Extend finalizes the final second or 24 frames of your video and continues the action.
Note that voice is not able to be effectively extended if it's not present in the last 1 second of video.

### Example prompts and output

This section presents several prompts, highlighting how descriptive details can elevate the outcome of each video.

#### Icicles

This video demonstrates how you can use the elements of prompt writing basics in your prompt.

#### Man on the phone

These videos demonstrate how you can revise your prompt with increasingly specific details to get Veo to refine the output to your liking.

#### Snow leopard

### Examples by writing elements

These examples show you how to refine your prompts by each basic element.

#### Subject and context

Specify the main focus (subject) and the background or environment (context).

#### Action

Specify what the subject is doing (e.g., walking, running, or turning their head).

#### Style

Add keywords to steer the generation toward a specific aesthetic (e.g., surreal, vintage, futuristic, film noir).

#### Camera motion and composition

Specify how the camera moves (POV shot, aerial view, tracking drone view) and how the shot is framed (wide shot, close-up, low angle).

#### Ambiance

Color palettes and lighting influence the mood. Try terms like "muted orange warm tones," "natural light," "sunrise," or "cool blue tones."

### Negative prompts

Negative prompts specify elements you don't want in the video.

- âŒ Don't use instructive language like no or don't. (e.g., "No walls").
- âœ… Do describe what you don't want to see. (e.g., "wall, frame").

### Aspect ratios

Veo lets you specify the aspect ratio for your video.

## Limitations

- Request latency: Min: 11 seconds; Max: 6 minutes (during peak hours).
- Regional limitations: In EU, UK, CH, MENA locations, the following
are the allowed values for personGeneration:

Veo 3: allow_adult only.
Veo 2: dont_allow and allow_adult. Default is dont_allow.

- Veo 3: allow_adult only.
- Veo 2: dont_allow and allow_adult. Default is dont_allow.
- Video retention: Generated videos are stored on the server for 2 days,
after which they are removed. To save a local copy, you must download your
video within 2 days of generation. Extended videos are treated as newly
generated videos.
- Watermarking: Videos created by Veo are watermarked using [SynthID](https://deepmind.google/science/synthid/), our tool for watermarking
and identifying AI-generated content. Videos can be verified using the
SynthID verification platform.
- Safety: Generated videos are passed through safety filters and
memorization checking processes that help mitigate privacy, copyright and
bias risks.
- Audio error: Veo 3.1 will sometimes block a video from generating
because of safety filters or other processing issues with the audio. You
will not be charged if your video is blocked from generating.

```
personGeneration
```

- Veo 3: allow_adult only.
- Veo 2: dont_allow and allow_adult. Default is dont_allow.

```
allow_adult
```

```
dont_allow
```

```
allow_adult
```

```
dont_allow
```

[SynthID](https://deepmind.google/technologies/synthid/)
[SynthID](https://deepmind.google/science/synthid/)

## Model features

[Preview](https://ai.google.dev/gemini-api/docs/models)
[Stable](https://ai.google.dev/gemini-api/docs/models)
[Stable](https://ai.google.dev/gemini-api/docs/models)

## Model versions

Check out the Pricing and [Rate limits](https://ai.google.dev/gemini-api/docs/rate-limits) pages for more Veo model-specific usage details.
Veo Fast versions allow developers to create videos with sound while maintaining high quality and optimizing for speed and business use cases. They're ideal for backend services that programmatically generate ads, tools for rapid A/B testing of creative concepts, or apps that need to quickly produce social media content.

### Veo 3.1 Preview

Gemini API
veo-3.1-generate-preview

```
veo-3.1-generate-preview
```

Input
Text, Image
Output
Video with audio
Text input
1,024 tokens
Output video
1

### Veo 3.1 Fast Preview

Gemini API
veo-3.1-fast-generate-preview

```
veo-3.1-fast-generate-preview
```

Input
Text, Image
Output
Video with audio
Text input
1,024 tokens
Output video
1

### Veo 3

Gemini API
veo-3.0-generate-001

```
veo-3.0-generate-001
```

Input
Text, Image
Output
Video with audio
Text input
1,024 tokens
Output video
1

### Veo 3 Fast

Gemini API
veo-3.0-fast-generate-001

```
veo-3.0-fast-generate-001
```

Input
Text, Image
Output
Video with audio
Text input
1,024 tokens
Output video
1

### Veo 2

Gemini API
veo-2.0-generate-001

```
veo-2.0-generate-001
```

Input
Text, image
Output
Video
Text input
N/A
Image input
Any image resolution and aspect ratio up to 20MB file size
Output video
Up to 2

## What's next

- Get started with the Veo 3.1 API by experimenting in the Veo Quickstart Colab
and the [Veo 3.1 applet](https://aistudio.google.com/apps/bundled/veo_studio).
- Learn how to write even better prompts with our [Introduction to prompt design](https://ai.google.dev/gemini-api/docs/prompting-intro).
[Veo Quickstart Colab](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_Veo.ipynb)
[Veo 3.1 applet](https://aistudio.google.com/apps/bundled/veo_studio)
[Introduction to prompt design](https://ai.google.dev/gemini-api/docs/prompting-intro)
Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-12-18 UTC.
-

        [Terms](https://policies.google.com/terms)

-

        [Privacy](https://policies.google.com/privacy)

-

        Manage cookies

[Terms](https://policies.google.com/terms)
[Privacy](https://policies.google.com/privacy)
Manage cookies
-

      [English](https://ai.google.dev/gemini-api/docs/video)

-
      [Deutsch](https://ai.google.dev/gemini-api/docs/video)

-
      [EspaÃ±ol â€“ AmÃ©rica Latina](https://ai.google.dev/gemini-api/docs/video)

-
      [FranÃ§ais](https://ai.google.dev/gemini-api/docs/video)

-
      [Indonesia](https://ai.google.dev/gemini-api/docs/video)

-
      [Italiano](https://ai.google.dev/gemini-api/docs/video)

-
      [Polski](https://ai.google.dev/gemini-api/docs/video)

-
      [PortuguÃªs â€“ Brasil](https://ai.google.dev/gemini-api/docs/video)

-
      [Shqip](https://ai.google.dev/gemini-api/docs/video)

-
      [TiÃªÌng ViÃªÌ£t](https://ai.google.dev/gemini-api/docs/video)

-
      [TÃ¼rkÃ§e](https://ai.google.dev/gemini-api/docs/video)

-
      [Ð ÑƒÑÑÐºÐ¸Ð¹](https://ai.google.dev/gemini-api/docs/video)

-
      [×¢×‘×¨×™×ª](https://ai.google.dev/gemini-api/docs/video)

-
      [Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‘Ø©](https://ai.google.dev/gemini-api/docs/video)

-
      [ÙØ§Ø±Ø³ÛŒ](https://ai.google.dev/gemini-api/docs/video)

-
      [à¤¹à¤¿à¤‚à¤¦à¥€](https://ai.google.dev/gemini-api/docs/video)

-
      [à¦¬à¦¾à¦‚à¦²à¦¾](https://ai.google.dev/gemini-api/docs/video)

-
      [à¸ à¸²à¸©à¸²à¹„à¸—à¸¢](https://ai.google.dev/gemini-api/docs/video)

-
      [ä¸­æ–‡ â€“ ç®€ä½“](https://ai.google.dev/gemini-api/docs/video)

-
      [ä¸­æ–‡ â€“ ç¹é«”](https://ai.google.dev/gemini-api/docs/video)

-
      [æ—¥æœ¬èªž](https://ai.google.dev/gemini-api/docs/video)

-
      [í•œêµ­ì–´](https://ai.google.dev/gemini-api/docs/video)

[English](https://ai.google.dev/gemini-api/docs/video)
[Deutsch](https://ai.google.dev/gemini-api/docs/video)
[EspaÃ±ol â€“ AmÃ©rica Latina](https://ai.google.dev/gemini-api/docs/video)
[FranÃ§ais](https://ai.google.dev/gemini-api/docs/video)
[Indonesia](https://ai.google.dev/gemini-api/docs/video)
[Italiano](https://ai.google.dev/gemini-api/docs/video)
[Polski](https://ai.google.dev/gemini-api/docs/video)
[PortuguÃªs â€“ Brasil](https://ai.google.dev/gemini-api/docs/video)
[Shqip](https://ai.google.dev/gemini-api/docs/video)
[TiÃªÌng ViÃªÌ£t](https://ai.google.dev/gemini-api/docs/video)
[TÃ¼rkÃ§e](https://ai.google.dev/gemini-api/docs/video)
[Ð ÑƒÑÑÐºÐ¸Ð¹](https://ai.google.dev/gemini-api/docs/video)
[×¢×‘×¨×™×ª](https://ai.google.dev/gemini-api/docs/video)
[Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‘Ø©](https://ai.google.dev/gemini-api/docs/video)
[ÙØ§Ø±Ø³ÛŒ](https://ai.google.dev/gemini-api/docs/video)
[à¤¹à¤¿à¤‚à¤¦à¥€](https://ai.google.dev/gemini-api/docs/video)
[à¦¬à¦¾à¦‚à¦²à¦¾](https://ai.google.dev/gemini-api/docs/video)
[à¸ à¸²à¸©à¸²à¹„à¸—à¸¢](https://ai.google.dev/gemini-api/docs/video)
[ä¸­æ–‡ â€“ ç®€ä½“](https://ai.google.dev/gemini-api/docs/video)
[ä¸­æ–‡ â€“ ç¹é«”](https://ai.google.dev/gemini-api/docs/video)
[æ—¥æœ¬èªž](https://ai.google.dev/gemini-api/docs/video)
[í•œêµ­ì–´](https://ai.google.dev/gemini-api/docs/video)
